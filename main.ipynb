{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a53dc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alessandro/Documents/Universita/Progetto_NLP/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from src.evaluate_crows import evaluate_crows\n",
    "from src.evaluate_stereoset import evaluate_stereoset\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f7aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_names = ['distilbert/distilbert-base-uncased','albert/albert-base-v2','FacebookAI/xlm-roberta-base']\n",
    "#models_names = ['albert/albert-base-v2','FacebookAI/xlm-roberta-base']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4305088d",
   "metadata": {},
   "source": [
    "# Crows evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841e4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert/albert-base-v2 were not used when initializing AlbertForMaskedLM: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CROWS for:  albert/albert-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1508/1508 [12:42<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-base were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing CROWS for:  FacebookAI/xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1508/1508 [27:41<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name in models_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    print('Computing CROWS for: ',model_name,)\n",
    "    results = evaluate_crows(model,tokenizer)\n",
    "    result_path = 'results/'+model_name.replace('/','-')\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    with open(os.path.join(result_path,'crows.pkl'),'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print('-'*25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62656df6",
   "metadata": {},
   "source": [
    "# StereoSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c04fac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 100%|██████████| 2106/2106 [00:00<00:00, 159559.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing StereoSet for:  distilbert/distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intra-sentence:   0%|          | 0/2106 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m model = AutoModelForMaskedLM.from_pretrained(model_name)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mComputing StereoSet for: \u001b[39m\u001b[33m'\u001b[39m,model_name,)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m results = \u001b[43mevaluate_stereoset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m result_path = \u001b[33m'\u001b[39m\u001b[33mresults/\u001b[39m\u001b[33m'\u001b[39m+model_name.replace(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m os.makedirs(result_path, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universita/Progetto_NLP/src/evaluate_stereoset.py:26\u001b[39m, in \u001b[36mevaluate_stereoset\u001b[39m\u001b[34m(model, tokenizer, ds, device)\u001b[39m\n\u001b[32m     24\u001b[39m ctx = ex[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# prendo i tre completamenti dal campo \"sentences\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m conts = \u001b[43m[\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     27\u001b[39m inputs = [(ctx + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + c).strip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m conts]\n\u001b[32m     29\u001b[39m enc = tokenizer(\n\u001b[32m     30\u001b[39m     inputs,\n\u001b[32m     31\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     33\u001b[39m     truncation=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     34\u001b[39m ).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universita/Progetto_NLP/src/evaluate_stereoset.py:26\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     24\u001b[39m ctx = ex[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# prendo i tre completamenti dal campo \"sentences\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m conts = [\u001b[43mopt\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m opt \u001b[38;5;129;01min\u001b[39;00m ex[\u001b[33m\"\u001b[39m\u001b[33msentences\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m     27\u001b[39m inputs = [(ctx + \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m + c).strip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m conts]\n\u001b[32m     29\u001b[39m enc = tokenizer(\n\u001b[32m     30\u001b[39m     inputs,\n\u001b[32m     31\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     padding=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     33\u001b[39m     truncation=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     34\u001b[39m ).to(device)\n",
      "\u001b[31mTypeError\u001b[39m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"McGill-NLP/stereoset\", \"intrasentence\", split=\"validation\")\n",
    "\n",
    "for model_name in models_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    print('Computing StereoSet for: ',model_name,)\n",
    "    results = evaluate_stereoset(model,tokenizer,dataset)\n",
    "    result_path = 'results/'+model_name.replace('/','-')\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    with open(os.path.join(result_path,'stereoset.pkl'),'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    print('-'*25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
